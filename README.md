# Overview

This project focuses on implementing a web crawler combined with the PageRank algorithm to rank web pages. The crawler is designed to traverse specific domains, extract URLs, and build a representation of the web graph. The PageRank algorithm is then applied to compute the importance scores of web pages based on their connectivity within the graph.

## Project Components

### 1. Crawler (crawler.py)

The crawler performs the following tasks:

- Starts with a set of seed URLs specified in a text file.
- Utilizes a breadth-first traversal strategy to explore web pages.
- Extracts URLs from anchor tags (`<a>` elements) within web pages.
- Verifies and filters URLs to ensure they belong to specified domains.
- Outputs crawled URLs and links between web pages.

### 2. PageRank Algorithm (pagerank.py)

The PageRank algorithm calculates importance scores for web pages based on their connectivity within the web graph. It includes the following steps:

- Reads crawled URLs and links output by the crawler.
- Builds a representation of the web graph using the collected data.
- Computes PageRank scores iteratively until convergence.
- Outputs the PageRank scores for each web page.

## Usage

To run the crawler and PageRank algorithm:

1. **Crawler:** Execute `crawler.py` with the following command:
   python3 crawler.py myseedURL.txt 2000

	•	myseedURL.txt: File containing seed URLs. For now, adding one url is sufficient. 

	•	2000: Maximum number of URLs to crawl.

2. **PageRank Algo Execution**: Execute pagerank.py with the following command:
   
   python3 pagerank.py crawler.output links.output 0.001

   	•	crawler.output: Output file generated by the crawler containing crawled URLs.

	•	links.output: Output file generated by the crawler containing links between web pages.

	•	0.001: Convergence threshold for PageRank computation.

## Notes 
	•	Ensure that the necessary dependencies (e.g., requests, BeautifulSoup) are installed before running the scripts.
	•	Adjust domain specifications and crawling parameters as needed for specific requirements.
	•	Warning: Exercise caution when using the crawler to avoid excessive pinging of websites, as it may lead to IP blacklisting.

## Contributing
Contributions are welcome! If you find any issues or have suggestions for improvements, feel free to open an issue or submit a pull request.

## License
This project is licensed under the MIT License.

## Contact
For any inquiries or questions, please contact andre.rodriguez9722@outlook.com
